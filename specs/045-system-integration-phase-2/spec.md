# Feature Specification: System Integration Layer — Phase 2 (Real AI Logic Activation)

**Feature Branch**: `045-system-integration-phase-2`
**Created**: 2025-01-27
**Status**: Draft
**Type**: ai-logic-activation
**Input**: User description: "Enable the FIRST working version of the full AI runtime: Real LLM calls (OpenAI, Gemini, DeepSeek), Real text-embedding generation, Real Qdrant similarity search, Real chapter-specific RAG context building, Real output generation for AI Blocks. This is the activation of ACTUAL intelligence across Chapters 1–3."

## User Scenarios & Testing *(mandatory)*

### User Story 1 - AI Blocks Produce Real Responses (Priority: P1)

As a user, I need AI blocks to produce real intelligent responses using LLM + Embeddings + Qdrant RAG pipelines, so I can get actual answers, explanations, quizzes, and diagrams based on chapter content.

**Why this priority**: This is the core value proposition of the interactive book. Without real AI logic, the system is just scaffolding and provides no actual value to users.

**Independent Test**: Can be fully tested by verifying real LLM responses are returned, real embeddings are generated, real Qdrant search works, and real AI block results are produced.

**Acceptance Scenarios**:

1. **Given** the feature is implemented, **When** I call ask-question endpoint with a question, **Then** I receive a real answer generated by LLM with RAG context from chapter content

2. **Given** the feature is implemented, **When** I call explain-like-10 endpoint with a concept, **Then** I receive a real simplified explanation generated by LLM with chapter context

3. **Given** the feature is implemented, **When** I call quiz endpoint with chapterId, **Then** I receive real quiz questions generated by LLM based on chapter learning objectives

4. **Given** the feature is implemented, **When** I call diagram endpoint with diagramType, **Then** I receive real diagram description/prompt generated by LLM

5. **Given** the feature is implemented, **When** I run CLI indexer for a chapter, **Then** chapter chunks are embedded and upserted into Qdrant successfully

---

### User Story 2 - Developer Can Index Chapters (Priority: P1)

As a developer, I need a CLI script to index chapters (generate embeddings and upsert into Qdrant), so I can populate the vector database with chapter content for RAG retrieval.

**Why this priority**: Without indexed chapters, RAG pipeline cannot retrieve relevant context. This is a prerequisite for real AI responses.

**Independent Test**: Can be fully tested by running CLI script and verifying embeddings are generated and vectors are upserted into Qdrant.

**Acceptance Scenarios**:

1. **Given** the feature is implemented, **When** I run `python backend/app/cli/index_chapter.py --chapter-id 1`, **Then** Chapter 1 chunks are embedded and upserted into Qdrant collection "chapter_1"

2. **Given** the feature is implemented, **When** I run `python backend/app/cli/index_chapter.py --chapter-id 2`, **Then** Chapter 2 chunks are embedded and upserted into Qdrant collection "chapter_2"

3. **Given** the feature is implemented, **When** I run `python backend/app/cli/index_chapter.py --chapter-id 3`, **Then** Chapter 3 chunks are embedded and upserted into Qdrant collection "chapter_3"

---

### Edge Cases

- What happens when OpenAI API fails?
  - **Expected**: Provider should handle gracefully, return error or fallback to default response
- What happens when Qdrant connection fails?
  - **Expected**: RAG pipeline should handle gracefully, return empty context or fallback
- What happens when embedding generation fails?
  - **Expected**: Embedding client should handle gracefully, return error or empty vector
- What happens when chapter chunks are empty?
  - **Expected**: Indexer should handle gracefully, log warning, skip indexing
- What happens when RAG search returns no results?
  - **Expected**: RAG pipeline should handle gracefully, return empty context or use fallback

## Requirements *(mandatory)*

### Functional Requirements

#### FR-001: Activate Providers (LLM)

- **FR-001.1**: System MUST update `backend/app/ai/providers/openai_provider.py`:
  - Implement real `generate()` method using OpenAI SDK
  - Use `settings.openai_api_key` for authentication
  - Use `settings.llm_model` for model selection (default: "gpt-4o-mini")
  - Handle OpenAI API errors gracefully
  - Return real LLM response with text and metadata
  - Add minimal error handling + TODO logging

- **FR-001.2**: System MUST update `backend/app/ai/providers/gemini_provider.py`:
  - Implement real `generate()` method using Gemini SDK
  - Use `settings.gemini_api_key` (or GEMINI_API_KEY env var) for authentication
  - Use `settings.llm_model` for model selection (default: "gemini-pro")
  - Handle Gemini API errors gracefully
  - Return real LLM response with text and metadata
  - Add minimal error handling + TODO logging

#### FR-002: Activate Embeddings

- **FR-002.1**: System MUST update `backend/app/ai/embeddings/embedding_client.py`:
  - Implement real `generate_embedding(text: str, chapter_id: int)` function:
    - Use OpenAI embeddings API (text-embedding-3-small)
    - Support chapter-specific embedding models (CH2_EMBEDDING_MODEL, CH3_EMBEDDING_MODEL)
    - Handle max token size (8191 for text-embedding-3-small)
    - Truncate text if exceeds max tokens
    - Return 1536-dimensional vector
    - Add error handling for API failures
  - Implement real `batch_embed(chunks: List[str])` function:
    - Use batch API endpoint for efficiency
    - Handle large batches (split if needed, e.g., 100 chunks per batch)
    - Return list of 1536-dimensional vectors
    - Add error handling for partial failures

#### FR-003: Qdrant Real Search Integration

- **FR-003.1**: System MUST update `backend/app/ai/rag/qdrant_store.py`:
  - Implement real `create_collection(collection_name: str)` function:
    - Use Qdrant client SDK
    - Use `settings.qdrant_url` and `settings.qdrant_api_key` for connection
    - Configure collection with vector size 1536 (for text-embedding-3-small)
    - Set distance metric to Cosine similarity
    - Configure HNSW index (m, ef_construct parameters)
    - Handle collection already exists error
    - Return True if successful, False otherwise
  - Implement real `upsert_vectors(collection_name: str, vectors: List[Dict])` function:
    - Use Qdrant client to batch upsert vectors
    - Vector structure: {id, vector (1536 dims), payload (metadata)}
    - Handle large batches (split if needed)
    - Return True if successful, False otherwise
  - Implement real `similarity_search(collection_name: str, query: str, top_k: int)` function:
    - Embed query text using embedding_client.generate_embedding()
    - Use Qdrant client to perform vector search
    - Return top_k results sorted by similarity score
    - Filter by chapter_id if provided
    - Return list of documents with id, score, payload

#### FR-004: Real RAG Pipeline

- **FR-004.1**: System MUST update `backend/app/ai/rag/pipeline.py`:
  - Implement real `run_rag_pipeline(query: str, chapter_id: int, top_k: int)` function:
    - Step 1: Load chapter metadata + chunks (from chapter_X_chunks.py)
    - Step 2: Embed user query (using embedding_client.generate_embedding())
    - Step 3: Perform Qdrant search (using qdrant_store.similarity_search())
    - Step 4: Build context window (assemble retrieved chunks into context string)
    - Step 5: Prepare final prompt (combine query + context)
    - Return context dictionary with context, chunks, query_embedding
    - Implement minimal, safe logic (no advanced ranking)

#### FR-005: AI Block Real Logic Activation

- **FR-005.1**: System MUST update `backend/app/ai/runtime/engine.py`:
  - Implement real flow for ask-question:
    - Call RAG pipeline to get context
    - Call LLM provider with prompt + context
    - Format response with answer and sources
  - Implement real flow for explain-like-10:
    - Call RAG pipeline to get context
    - Call LLM provider with ELI10 prompt + context
    - Format response with explanation, analogies, examples
  - Implement real flow for quiz:
    - Call RAG pipeline to get context (all sections)
    - Call LLM provider with quiz generation prompt + context
    - Format response with quiz questions, answers, explanations
  - Implement real flow for diagram:
    - Call RAG pipeline to get context (optional)
    - Call LLM provider with diagram generation prompt + context
    - Format response with diagram description/prompt
  - Connect to: RAG pipeline, LLM provider call, output formatters

#### FR-006: Skill-Based Real Implementations

- **FR-006.1**: System MUST update `backend/app/ai/skills/retrieval_skill.py` (if exists):
  - Implement real retrieval logic
  - Call RAG pipeline to get context
  - Return retrieved context

- **FR-006.2**: System MUST update `backend/app/ai/skills/formatting_skill.py` (if exists):
  - Implement real formatting logic
  - Format LLM response for frontend
  - Return formatted response

- **FR-006.3**: System MUST update `backend/app/ai/skills/prompt_builder_skill.py` (if exists):
  - Implement real prompt building logic
  - Build prompts for different block types
  - Return formatted prompt

#### FR-007: Subagent Activation

- **FR-007.1**: System MUST activate real logic in Chapter 3 subagents:
  - `backend/app/ai/subagents/ch3/ask_question_agent.py`:
    - Use retrieval_skill to get context
    - Use prompt_builder_skill to build prompt
    - Call LLM provider
    - Use formatting_skill to format response
  - `backend/app/ai/subagents/ch3/explain_el10_agent.py`:
    - Use retrieval_skill to get context
    - Use prompt_builder_skill to build ELI10 prompt
    - Call LLM provider
    - Use formatting_skill to format response
  - `backend/app/ai/subagents/ch3/quiz_agent.py`:
    - Use retrieval_skill to get context (all sections)
    - Use prompt_builder_skill to build quiz prompt
    - Call LLM provider
    - Use formatting_skill to format response
  - `backend/app/ai/subagents/ch3/diagram_agent.py`:
    - Use retrieval_skill to get context (optional)
    - Use prompt_builder_skill to build diagram prompt
    - Call LLM provider
    - Use formatting_skill to format response

#### FR-008: Chapter RAG Indexing Command

- **FR-008.1**: System MUST create `backend/app/cli/index_chapter.py`:
  - Function: `index_chapter(chapter_id: int)`
  - Steps:
    1. Read chapter chunks from chapter_X_chunks.py
    2. Generate embeddings using embedding_client.batch_embed()
    3. Upsert into Qdrant using qdrant_store.upsert_vectors()
    4. Log progress and results
  - Support command-line arguments: `--chapter-id`, `--collection-name`
  - Add error handling and logging

---

## Non-Functional Requirements

- **NFR-001**: All implementations MUST be minimal but fully functional
- **NFR-002**: All implementations MUST include error handling
- **NFR-003**: All implementations MUST include TODO logging
- **NFR-004**: No complex ranking, caching, or parallelization (keep it simple)
- **NFR-005**: No diagram generation logic (placeholder only)
- **NFR-006**: No advanced quiz logic (basic generation only)

---

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Real LLM responses returned through runtime engine
- **SC-002**: Real embeddings and Qdrant search working
- **SC-003**: Real AI Block results produced
- **SC-004**: CLI script indexes all chapters successfully
- **SC-005**: No broken imports, no missing modules
- **SC-006**: All error handling in place

---

## Constraints *(mandatory)*

### Technical Constraints

- **C-001**: MUST NOT implement complex ranking algorithms
- **C-002**: MUST NOT implement caching
- **C-003**: MUST NOT implement parallelization
- **C-004**: MUST NOT implement diagram generation logic (placeholder only)
- **C-005**: MUST NOT implement advanced quiz logic (basic generation only)
- **C-006**: MUST keep implementations minimal but functional
- **C-007**: MUST include error handling for all API calls

### Process Constraints

- **C-008**: MUST follow SDD workflow: spec → plan → tasks → implementation
- **C-009**: MUST create PHR after specification completion
- **C-010**: MUST validate against Constitutional principles before marking complete

### Scope Constraints (Out of Scope)

- **OOS-001**: Complex ranking algorithms
- **OOS-002**: Caching layer
- **OOS-003**: Parallelization
- **OOS-004**: Diagram generation logic (real diagram creation)
- **OOS-005**: Advanced quiz logic (difficulty adjustment, adaptive questions)
- **OOS-006**: Response streaming
- **OOS-007**: Rate limiting
- **OOS-008**: Cost tracking

---

## Dependencies *(mandatory)*

### Internal Dependencies

- **D-001**: Feature 044 (System Integration Phase 1) MUST be complete
- **D-002**: Feature 005 (AI Runtime Engine) MUST be complete
- **D-003**: Feature 040 (Chapter 3 RAG + Runtime Integration) MUST be complete
- **D-004**: Feature 041 (Chapter 3 Subagents + Skills) MUST be complete
- **D-005**: All chapter chunk files MUST exist (chapter_1_chunks.py, chapter_2_chunks.py, chapter_3_chunks.py)

### External Dependencies

- **D-006**: OpenAI Python SDK (`openai` package)
- **D-007**: Google Generative AI SDK (`google-generativeai` package)
- **D-008**: Qdrant Python Client (`qdrant-client` package)
- **D-009**: Environment variables: `OPENAI_API_KEY`, `GEMINI_API_KEY` (optional), `QDRANT_URL`, `QDRANT_API_KEY`

### Blocking Issues

- None identified. All dependencies resolved.

### Assumptions

- **A-001**: OpenAI API key is available in environment
- **A-002**: Qdrant instance is accessible (local or cloud)
- **A-003**: Chapter chunks are available for indexing
- **A-004**: Embedding models are accessible (OpenAI embeddings API)

---

## Implementation Notes *(optional guidance)*

### Recommended Implementation Order

1. **Phase 1: Provider Activation**
   - Implement OpenAI provider
   - Implement Gemini provider
   - Test provider calls

2. **Phase 2: Embedding Activation**
   - Implement generate_embedding()
   - Implement batch_embed()
   - Test embedding generation

3. **Phase 3: Qdrant Integration**
   - Implement create_collection()
   - Implement upsert_vectors()
   - Implement similarity_search()
   - Test Qdrant operations

4. **Phase 4: RAG Pipeline**
   - Implement run_rag_pipeline()
   - Test RAG pipeline end-to-end

5. **Phase 5: Skills Activation**
   - Implement retrieval_skill
   - Implement prompt_builder_skill
   - Implement formatting_skill

6. **Phase 6: Subagent Activation**
   - Activate Chapter 3 subagents
   - Test subagent calls

7. **Phase 7: Runtime Engine**
   - Activate real flow in engine.py
   - Test AI block endpoints

8. **Phase 8: CLI Indexer**
   - Create index_chapter.py
   - Test indexing for all chapters

---

**Next Steps**: Proceed to `/sp.plan` to create architectural plan for real AI logic activation.

