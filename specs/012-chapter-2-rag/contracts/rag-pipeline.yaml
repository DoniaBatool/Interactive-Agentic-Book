# RAG Pipeline Contract for Chapter 2

**Feature**: 012-chapter-2-rag
**Created**: 2025-12-05
**Status**: Draft

## Overview

This contract defines the RAG (Retrieval-Augmented Generation) pipeline interface for Chapter 2. The pipeline orchestrates chunk retrieval, query embedding, vector search, and context assembly for ROS 2 knowledge retrieval.

## Pipeline Flow

```
User Query (ROS 2 question)
    ↓
[Step 1: Load Chapter 2 Chunks]
    ↓
[Step 2: Embed Query]
    ↓
[Step 3: Perform Vector Search in Qdrant]
    ↓
[Step 4: Build Retrieval Context]
    ↓
[Step 5: Return Context to Runtime Engine]
    ↓
Runtime Engine → Subagents → LLM Provider
```

## Step 1: Load Chapter 2 Chunks

**Function**: `get_chapter_chunks(chapter_id: int = 2) -> List[Dict[str, Any]]`

**Input**:
- `chapter_id`: Chapter identifier (default: 2)

**Output**:
```yaml
chunks:
  - id: str                    # Unique chunk ID (e.g., "ch2-s1-c0")
    text: str                   # Chunk text content
    chapter_id: int             # Chapter identifier (2)
    section_id: str             # Section identifier (e.g., "introduction-to-ros2")
    position: int               # Position in chapter (0-based)
    word_count: int             # Word count
    metadata:
      heading: str              # Section heading
      type: str                 # "paragraph", "heading", "glossary", etc.
      has_diagram: bool         # True if section has diagram placeholder
      has_ai_block: bool         # True if section has AI block
```

**Error Handling**:
- If chunks not yet implemented: Return empty list `[]`
- If chapter_id invalid: Return empty list `[]`
- Log warning if chunks unavailable

## Step 2: Embed Query

**Function**: `generate_embedding(text: str) -> List[float]`

**Input**:
- `text`: User query text (e.g., "What is a ROS 2 node?")

**Output**:
```yaml
embedding: List[float]          # Embedding vector (e.g., [0.123, -0.456, 0.789, ...])
                                # Dimension depends on model (e.g., 1536 for text-embedding-3-small)
```

**Error Handling**:
- If embedding model not configured: Return empty list `[]`
- If API call fails: Return empty list `[]` and log error
- If text exceeds max token size: Truncate and log warning

## Step 3: Perform Vector Search

**Function**: `similarity_search(collection_name: str, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]`

**Input**:
- `collection_name`: Qdrant collection name (e.g., "chapter_2")
- `query_embedding`: Query embedding vector from Step 2
- `top_k`: Number of results to return (default: 5)

**Output**:
```yaml
results:
  - id: str                     # Document ID
    score: float                 # Similarity score (0.0-1.0, highest first)
    payload:
      text: str                  # Original text chunk
      chapter_id: int            # Chapter identifier (2)
      section_id: str            # Section identifier
      position: int              # Position in chapter
      metadata:
        heading: str
        type: str
        has_diagram: bool
        has_ai_block: bool
```

**Error Handling**:
- If collection doesn't exist: Return empty list `[]` and log error
- If Qdrant connection fails: Return empty list `[]` and log error
- If query_embedding is empty: Return empty list `[]` and log warning

## Step 4: Build Retrieval Context

**Function**: Internal step in `run_rag_pipeline()`

**Input**:
- `results`: Retrieved chunks from Step 3

**Output**:
```yaml
context:
  context: str                   # Assembled context string (concatenated chunk texts)
  chunks: List[Dict[str, Any]]  # Retrieved chunks with metadata
  query_embedding: List[float]   # Query embedding vector
```

**Context Assembly Rules** (TODO):
- Concatenate chunk texts in order of similarity score
- Add section headers for context
- Limit total context size (max tokens/configurable)
- Include metadata in context string

## Step 5: Return Context to Runtime Engine

**Function**: `run_rag_pipeline(query: str, chapter_id: int, top_k: int = 5) -> Dict[str, Any]`

**Input**:
- `query`: User query text
- `chapter_id`: Chapter identifier (2 for Chapter 2)
- `top_k`: Number of chunks to retrieve (default: 5)

**Output**:
```yaml
response:
  context: str                   # Assembled context string
  chunks: List[Dict[str, Any]]   # Retrieved chunks with metadata
  query_embedding: List[float]    # Query embedding vector
```

**Error Handling**:
- If any step fails: Return empty context with error message
- Log errors at each step
- Return partial results if possible

## Chapter 2 Specific Considerations

- **ROS 2 Concepts**: Pipeline should retrieve chunks related to ROS 2 concepts (nodes, topics, services, actions, packages, launch files)
- **Section Filtering**: If `sectionId` provided in request, filter chunks by section_id
- **Context Relevance**: Prioritize chunks from relevant sections (e.g., "introduction-to-ros2" for ROS 2 questions)
- **Metadata Usage**: Use chunk metadata (has_diagram, has_ai_block) to provide richer context

## Integration Points

- **Runtime Engine**: `run_rag_pipeline()` is called by `run_ai_block()` in `engine.py`
- **Subagents**: Context from RAG pipeline is passed to subagents (ask_question_agent, explain_el10_agent, quiz_agent, diagram_agent)
- **LLM Provider**: Subagents use context in LLM prompts for generating responses

## Status

⚠️ **All functions are placeholders with TODO markers. No real implementation exists.**
